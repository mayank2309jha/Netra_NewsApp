{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1b8f6b-4070-4be6-9829-5ba72b5e1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------------------\n",
    "# LOAD HEADERS\n",
    "# -----------------------------------------\n",
    "def fill_headers_list(headers_list):\n",
    "    with open('headers.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            headers = {k: v for k, v in row.items() if v}\n",
    "            headers_list.append(headers)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# FETCH SOUP WITH RANDOM HEADERS\n",
    "# -----------------------------------------\n",
    "def get_soup(url, headers_list):\n",
    "    headers = random.choice(headers_list)\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# GOOGLE NEWS CARD EXTRACTION\n",
    "# -----------------------------------------\n",
    "def get_cards_by_headers(soup):\n",
    "    cards = soup.find_all(\"c-wiz\", {\n",
    "        \"jsrenderer\": \"ARwRbe\",\n",
    "        \"jsmodel\": \"hc6Ubd\",\n",
    "        \"class\": \"PO9Zff Ccj79 kUVvS\"\n",
    "    })\n",
    "    return cards\n",
    "\n",
    "\n",
    "def get_news_cards(cards):\n",
    "    return [str(card) for card in cards]\n",
    "\n",
    "\n",
    "def peel_outer_html(cards):\n",
    "    inner_divs = []\n",
    "    for card in cards:\n",
    "        inner_cwiz = card.find(\"c-wiz\", recursive=True)\n",
    "        if inner_cwiz:\n",
    "            first_div = inner_cwiz.find(\"div\", recursive=False)\n",
    "            if first_div:\n",
    "                second_div = first_div.find(\"div\", recursive=False)\n",
    "                if second_div:\n",
    "                    inner_divs.append(second_div)\n",
    "    return inner_divs\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# DOM CONVERSION\n",
    "# -----------------------------------------\n",
    "def element_to_dom(element):\n",
    "    \"\"\"Recursively convert a BeautifulSoup element to a nested dictionary.\"\"\"\n",
    "    dom = {}\n",
    "\n",
    "    for child in element.children:\n",
    "        if child.name is None:\n",
    "            text = child.strip()\n",
    "            if text:\n",
    "                dom[\"text\"] = text\n",
    "            continue\n",
    "\n",
    "        tag = child.name\n",
    "\n",
    "        if tag == \"a\":\n",
    "            value = child.get(\"href\")\n",
    "\n",
    "        elif tag == \"img\":\n",
    "            # Try multiple possibilities\n",
    "            value = (\n",
    "                child.get(\"srcset\") or\n",
    "                child.get(\"data-src\") or\n",
    "                child.get(\"src\") or\n",
    "                \"N/A\"\n",
    "            )\n",
    "            # Extract first gstatic URL if srcset\n",
    "            if \"gstatic\" in value:\n",
    "                value = value.split(\" \")[0]\n",
    "\n",
    "        elif tag == \"svg\":\n",
    "            value = str(child)\n",
    "\n",
    "        elif tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"span\", \"time\", \"button\"]:\n",
    "            value = child.decode_contents().strip()\n",
    "\n",
    "        else:\n",
    "            nested = element_to_dom(child)\n",
    "            value = nested if nested else child.decode_contents().strip()\n",
    "\n",
    "        if tag in dom:\n",
    "            if isinstance(dom[tag], list):\n",
    "                dom[tag].append(value)\n",
    "            else:\n",
    "                dom[tag] = [dom[tag], value]\n",
    "        else:\n",
    "            dom[tag] = value\n",
    "\n",
    "    return dom\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# REAL IMAGE SCRAPER (FROM PUBLISHER SITE)\n",
    "# -----------------------------------------\n",
    "def get_real_image_from_article(url, headers_list):\n",
    "    \"\"\"\n",
    "    Fetch the article page and extract the REAL OG image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = get_soup(url, headers_list)\n",
    "\n",
    "        og = soup.find(\"meta\", property=\"og:image\")\n",
    "        if og and og.get(\"content\"):\n",
    "            return og.get(\"content\")\n",
    "\n",
    "        tw = soup.find(\"meta\", property=\"twitter:image\")\n",
    "        if tw and tw.get(\"content\"):\n",
    "            return tw.get(\"content\")\n",
    "\n",
    "        # fallback: any <img>\n",
    "        main_img = soup.select_one(\"img[src]\")\n",
    "        if main_img:\n",
    "            return main_img[\"src\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Error fetching real image:\", e)\n",
    "\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# STRUCTURED DATA EXTRACTION\n",
    "# -----------------------------------------\n",
    "def get_news_data(dom_structure, i, inner_divs_html, headers_list):\n",
    "    BASE_URL = \"https://news.google.com\"\n",
    "    today_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    def get_headline(i):\n",
    "        html = inner_divs_html[i]\n",
    "        temp_soup = BeautifulSoup(html, 'html.parser')\n",
    "        button = temp_soup.find('button')\n",
    "        return button['aria-label'] if button and button.has_attr('aria-label') else \"N/A\"\n",
    "\n",
    "    # extract primary block\n",
    "    article = dom_structure.get(\"div\", {}).get(\"article\", {})\n",
    "    headline = get_headline(i)\n",
    "\n",
    "    # Build Primary Article Data\n",
    "    raw_link = article.get('a', '')\n",
    "    article_link = BASE_URL + raw_link if raw_link else \"N/A\"\n",
    "\n",
    "    real_image = (\n",
    "        get_real_image_from_article(article_link, headers_list)\n",
    "        if article_link != \"N/A\"\n",
    "        else \"N/A\"\n",
    "    )\n",
    "\n",
    "    news_data = {\n",
    "        'headline': headline,\n",
    "        'author': article.get('div', [])[2].get('div', {}).get('span', 'N/A')\n",
    "        if isinstance(article.get('div', []), list) and len(article.get('div', [])) > 2\n",
    "        else 'N/A',\n",
    "\n",
    "        'article_link': article_link,\n",
    "        'featured_image': real_image,\n",
    "\n",
    "        'source_logo': (\n",
    "            article.get('div', [])[1].get('div', [])[0].get('img', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'source_name': (\n",
    "            article.get('div', [])[1]\n",
    "            .get('div', [])[0]\n",
    "            .get('div', {})\n",
    "            .get('div', {})\n",
    "            .get('text', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'publish_date': today_date\n",
    "    }\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # RELATED ARTICLES\n",
    "    # -----------------------------------------\n",
    "    related_articles = []\n",
    "    possible_related = dom_structure.get(\"div\", {}).get(\"div\", {}).get(\"article\", [])\n",
    "\n",
    "    if isinstance(possible_related, list):\n",
    "        for ra in possible_related:\n",
    "            if not isinstance(ra, dict):\n",
    "                continue\n",
    "\n",
    "            divs = ra.get('div', [])\n",
    "            if not isinstance(divs, list) or len(divs) < 3:\n",
    "                continue\n",
    "\n",
    "            raw_link = ra.get('a', '')\n",
    "            full_link = BASE_URL + raw_link if raw_link else \"N/A\"\n",
    "\n",
    "            # REAL IMAGE FOR RELATED\n",
    "            ra_image = (\n",
    "                get_real_image_from_article(full_link, headers_list)\n",
    "                if full_link != \"N/A\"\n",
    "                else \"N/A\"\n",
    "            )\n",
    "\n",
    "            article_data = {\n",
    "                'headline': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'author': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'article_link': full_link,\n",
    "                'featured_image': ra_image,\n",
    "                'source_logo': (\n",
    "                    divs[1].get('div', [])[0].get('img', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "                'source_name': (\n",
    "                    divs[1].get('div', [])[0]\n",
    "                    .get('div', {})\n",
    "                    .get('div', {})\n",
    "                    .get('text', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "\n",
    "                'publish_date': today_date\n",
    "            }\n",
    "\n",
    "            related_articles.append(article_data)\n",
    "\n",
    "    # Final Combined Output\n",
    "    return {\n",
    "        'primary_article': news_data,\n",
    "        'related_articles': related_articles,\n",
    "        'total_related_articles': len(related_articles)\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# SCRAPE FUNCTION\n",
    "# -----------------------------------------\n",
    "def scrape(url, file):\n",
    "    headers_list = []\n",
    "    fill_headers_list(headers_list)\n",
    "\n",
    "    soup = get_soup(url, headers_list)\n",
    "    cards = get_cards_by_headers(soup)\n",
    "    inner_divs = peel_outer_html(cards)\n",
    "    inner_divs_html = [str(div) for div in inner_divs]\n",
    "\n",
    "    dom_structures = []\n",
    "    for html in inner_divs_html:\n",
    "        temp_soup = BeautifulSoup(html, \"html.parser\")\n",
    "        dom_dict = element_to_dom(temp_soup)\n",
    "        dom_structures.append(dom_dict)\n",
    "\n",
    "    news_data = []\n",
    "\n",
    "    for i, dom in enumerate(dom_structures):\n",
    "        try:\n",
    "            news_item = get_news_data(dom, i, inner_divs_html, headers_list)\n",
    "            news_data.append(news_item)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing item {i + 1}:\", e)\n",
    "\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\n✅ File saved:\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff666c2d-db9c-4149-90a8-e0c0c14b5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ File saved: india_news.json\n"
     ]
    }
   ],
   "source": [
    "india = \"https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRE55YXpBU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(india, 'india_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbab2b7-08bc-412a-9ffa-5f25450ec3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

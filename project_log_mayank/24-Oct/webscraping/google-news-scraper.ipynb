{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc29294-aed2-40b5-8dbd-47d6f6a42346",
   "metadata": {},
   "source": [
    "## Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "34d5cdae-af99-493f-ab40-7144b6653140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a08d60-71a3-4cdf-9080-c440822c78e8",
   "metadata": {},
   "source": [
    "## Get everything into the header list + Soup Content + News Cards in an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1f79e-47fe-43d3-85ea-8fb9c2cfca12",
   "metadata": {},
   "source": [
    "## Then Remove the outer HTML of the news cards and reduce them to their essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eee77365-50ce-4291-98c0-62ff9a18ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_headers_list(headers_list):\n",
    "    with open('headers.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            # Remove empty values to avoid sending None/empty headers\n",
    "            headers = {k: v for k, v in row.items() if v}\n",
    "            headers_list.append(headers)\n",
    "def get_soup(url, headers_list):\n",
    "    headers = random.choice(headers_list)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup\n",
    "def get_cards_by_headers(soup):\n",
    "    cards = soup.find_all(\"c-wiz\", {\n",
    "        \"jsrenderer\": \"ARwRbe\",\n",
    "        \"jsmodel\": \"hc6Ubd\",\n",
    "        \"class\": \"PO9Zff Ccj79 kUVvS\"\n",
    "    })\n",
    "    return cards\n",
    "def get_news_cards(cards):\n",
    "    return [str(card) for card in cards]\n",
    "def peel_outer_html(cards):\n",
    "    inner_divs = []\n",
    "    for card in cards:\n",
    "        inner_cwiz = card.find(\"c-wiz\", recursive=True)\n",
    "        if inner_cwiz:\n",
    "            first_div = inner_cwiz.find(\"div\", recursive=False)\n",
    "            if first_div:\n",
    "                second_div = first_div.find(\"div\", recursive=False)\n",
    "                if second_div:\n",
    "                    inner_divs.append(second_div)\n",
    "    return inner_divs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59e359-b806-48d4-9196-f91974d99c5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## This is function which takes a card's HTML and returns a dictionary of dictionaries which is arranged like the DOM's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c60d314a-2e32-4f11-be6b-5525ee48104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_to_dom(element):\n",
    "    \"\"\"Recursively convert a BeautifulSoup element to a nested dictionary structure.\"\"\"\n",
    "    dom = {}\n",
    "\n",
    "    for child in element.children:\n",
    "        # Skip text nodes that are just whitespace\n",
    "        if child.name is None:\n",
    "            text = child.strip()\n",
    "            if text:\n",
    "                dom[\"text\"] = text\n",
    "            continue\n",
    "\n",
    "        tag = child.name\n",
    "\n",
    "        # Determine value based on tag type\n",
    "        if tag == \"a\":\n",
    "            value = child.get(\"href\")\n",
    "        elif tag == \"img\":\n",
    "            value = child.get(\"src\")\n",
    "        elif tag == \"svg\":\n",
    "            value = str(child)\n",
    "        elif tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"span\", \"time\", \"button\"]:\n",
    "            value = child.decode_contents().strip()\n",
    "        else:\n",
    "            # If element has nested tags, recurse\n",
    "            nested = element_to_dom(child)\n",
    "            value = nested if nested else child.decode_contents().strip()\n",
    "\n",
    "        # Handle multiple same-tag children by converting value into a list\n",
    "        if tag in dom:\n",
    "            if isinstance(dom[tag], list):\n",
    "                dom[tag].append(value)\n",
    "            else:\n",
    "                dom[tag] = [dom[tag], value]\n",
    "        else:\n",
    "            dom[tag] = value\n",
    "\n",
    "    return dom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d835b-99b1-4eeb-b174-46b39980e2a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## This function takes a dictionary of dictionaries and returns the data in more usable format which will be the format of the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fd908adf-3e87-4c4b-b2c9-eaefb452fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(dom_structure, i):\n",
    "    \"\"\"\n",
    "    Extracts structured news data from a single dom_structure[i] element.\n",
    "    Returns a dictionary containing:\n",
    "      - primary_article (headline, link, author, etc.)\n",
    "      - related_articles (list of dictionaries)\n",
    "      - total_related_articles (count)\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://news.google.com\"\n",
    "    today_date = datetime.now().strftime(\"%d-%m-%Y\")  # current date in dd-mm-yyyy\n",
    "\n",
    "    def get_headline(i):\n",
    "        html = inner_divs_html[i]\n",
    "        temp_soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find the button\n",
    "        button = temp_soup.find('button')\n",
    "        \n",
    "        # Get the aria-label value\n",
    "        if button and button.has_attr('aria-label'):\n",
    "            return button['aria-label']\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    # --- PRIMARY ARTICLE ---\n",
    "    article = dom_structure.get(\"div\", {}).get(\"article\", {})\n",
    "    headline = get_headline(i)\n",
    "    news_data = {\n",
    "        'headline': headline if headline is not None else 'N/A',\n",
    "\n",
    "        'author': article.get('div', [])[2].get('div', {}).get('span', 'N/A')\n",
    "        if isinstance(article.get('div', []), list) and len(article.get('div', [])) > 2 else 'N/A',\n",
    "\n",
    "        'article_link': (\n",
    "            BASE_URL + article.get('a', '')\n",
    "            if article.get('a') not in (None, 'N/A') else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'featured_image': article.get('figure', {}).get('img', 'N/A'),\n",
    "\n",
    "        'source_logo': (\n",
    "            article.get('div', [])[1].get('div', [])[0].get('img', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'source_name': (\n",
    "            article.get('div', [])[1]\n",
    "            .get('div', [])[0]\n",
    "            .get('div', {})\n",
    "            .get('div', {})\n",
    "            .get('text', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        # --- NEW FIELD ---\n",
    "        'publish_date': today_date\n",
    "    }\n",
    "\n",
    "    # --- RELATED ARTICLES ---\n",
    "    related_articles = []\n",
    "    possible_related = dom_structure.get(\"div\", {}).get(\"div\", {}).get(\"article\", [])\n",
    "\n",
    "    if isinstance(possible_related, list):\n",
    "        for article in possible_related:\n",
    "            if not isinstance(article, dict):\n",
    "                continue\n",
    "\n",
    "            divs = article.get('div', [])\n",
    "            if not isinstance(divs, list) or len(divs) < 3:\n",
    "                continue\n",
    "\n",
    "            raw_link = article.get('a', '')\n",
    "            full_link = BASE_URL + raw_link if raw_link not in (None, 'N/A') else 'N/A'\n",
    "\n",
    "            article_data = {\n",
    "                'headline': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'author': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'article_link': full_link,\n",
    "                'source_logo': (\n",
    "                    divs[1].get('div', [])[0].get('img', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "                'source_name': (\n",
    "                    divs[1].get('div', [])[0]\n",
    "                    .get('div', {})\n",
    "                    .get('div', {})\n",
    "                    .get('text', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "\n",
    "                # --- NEW FIELD ---\n",
    "                'publish_date': today_date\n",
    "            }\n",
    "            related_articles.append(article_data)\n",
    "\n",
    "    # --- FINAL OUTPUT ---\n",
    "    complete_news_data = {\n",
    "        'primary_article': news_data,\n",
    "        'related_articles': related_articles,\n",
    "        'total_related_articles': len(related_articles)\n",
    "    }\n",
    "\n",
    "    return complete_news_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3ee1f-2622-480a-8d38-1970fc39c655",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The Main function which is going to perform the scraping and then save it in a given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9f7e3132-c968-4234-aaa9-5b1b176bb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url, file):\n",
    "    headers_list = []\n",
    "    fill_headers_list(headers_list)\n",
    "    soup = get_soup(url, headers_list)\n",
    "    cards = get_cards_by_headers(soup)\n",
    "    news_cards = get_news_cards(cards)\n",
    "    inner_divs = peel_outer_html(cards)\n",
    "    inner_divs_html = [str(div) for div in inner_divs]\n",
    "    dom_structures = []\n",
    "    for html in inner_divs_html:\n",
    "        temp_soup = BeautifulSoup(html, \"html.parser\")\n",
    "        dom_dict = element_to_dom(temp_soup)\n",
    "        dom_structures.append(dom_dict)\n",
    "    news_data = []\n",
    "\n",
    "    for i, dom in enumerate(dom_structures):\n",
    "        try:\n",
    "            news_item = get_news_data(dom, i)\n",
    "            news_data.append(news_item)\n",
    "            # print(f\"Processed item {i + 1}/{len(dom_structures)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {i + 1}: {e}\")\n",
    "    \n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_data, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\n✅ File \"+file+\" saved successfully in the working directory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbf0a0-e29b-43d8-8119-bafbcad3c764",
   "metadata": {},
   "source": [
    "## Scrape it all finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "113360c5-6477-45da-839b-f4e8582de667",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://news.google.com\"\n",
    "\n",
    "def safe_text(el):\n",
    "    \"\"\"Return stripped text or None.\"\"\"\n",
    "    if not el:\n",
    "        return None\n",
    "    txt = el.get_text(separator=\" \", strip=True)\n",
    "    return txt if txt else None\n",
    "\n",
    "def make_full_link(href):\n",
    "    if not href:\n",
    "        return None\n",
    "    # Google News often uses relative links like \"./articles/...\"\n",
    "    if href.startswith(\"http\"):\n",
    "        return href\n",
    "    if href.startswith(\"./\"):\n",
    "        href = href[1:]  # remove leading dot to make it relative to BASE_URL\n",
    "    return BASE_URL + href\n",
    "\n",
    "def parse_card(card_tag):\n",
    "    \"\"\"Extract structured data from a single c-wiz / card Tag.\"\"\"\n",
    "    today_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    data = {\n",
    "        \"primary_article\": {\n",
    "            \"headline\": \"N/A\",\n",
    "            \"author\": \"N/A\",\n",
    "            \"article_link\": \"N/A\",\n",
    "            \"featured_image\": \"N/A\",\n",
    "            \"source_logo\": \"N/A\",\n",
    "            \"source_name\": \"N/A\",\n",
    "            \"publish_date\": today_date\n",
    "        },\n",
    "        \"related_articles\": [],\n",
    "        \"total_related_articles\": 0\n",
    "    }\n",
    "\n",
    "    # 1) HEADLINE: Often stored in a button's aria-label (or within an <a> or <h3>)\n",
    "    button = card_tag.find(\"button\", attrs={\"aria-label\": True})\n",
    "    if button:\n",
    "        data[\"primary_article\"][\"headline\"] = button[\"aria-label\"]\n",
    "\n",
    "    # fallback: try article heading text\n",
    "    if data[\"primary_article\"][\"headline\"] in (None, \"N/A\"):\n",
    "        # common heading selectors\n",
    "        h = card_tag.select_one(\"h3, h4, .DY5T1d\")  # class used by google news headlines sometimes\n",
    "        if h:\n",
    "            val = safe_text(h)\n",
    "            if val:\n",
    "                data[\"primary_article\"][\"headline\"] = val\n",
    "\n",
    "    # 2) ARTICLE LINK: look for first <a> inside an article or heading\n",
    "    a = card_tag.find(\"a\", href=True)\n",
    "    if a:\n",
    "        data[\"primary_article\"][\"article_link\"] = make_full_link(a[\"href\"])\n",
    "\n",
    "    # 3) FEATURED IMAGE: look for <img> within figure or img tag\n",
    "    img = card_tag.find(\"img\", src=True)\n",
    "    if img and img.get(\"src\"):\n",
    "        data[\"primary_article\"][\"featured_image\"] = img[\"src\"]\n",
    "\n",
    "    # 4) SOURCE NAME & LOGO: sometimes within a nested div or span\n",
    "    # Try a few heuristics:\n",
    "    # - look for an element that looks like a source name\n",
    "    source_name = None\n",
    "    # Common pattern: span class that includes source name or small text near top\n",
    "    # try: small or .wEwyrc or .QmrVtf etc (Google uses many classes)\n",
    "    possible = card_tag.select(\"div, span\")\n",
    "    for el in possible[:25]:  # limit search area\n",
    "        txt = safe_text(el)\n",
    "        # simple heuristic: short text (<=5 words) and not the headline\n",
    "        if txt and len(txt.split()) <= 6 and txt != data[\"primary_article\"][\"headline\"]:\n",
    "            # also avoid timestamps like \"2 hours ago\" — choose ones without 'ago' or digits+hr\n",
    "            if \"ago\" in txt.lower() or any(tok.endswith(\"h\") for tok in txt.split()):\n",
    "                continue\n",
    "            source_name = txt\n",
    "            break\n",
    "    if source_name:\n",
    "        data[\"primary_article\"][\"source_name\"] = source_name\n",
    "\n",
    "    # try to find a small image that could be the logo\n",
    "    logo = None\n",
    "    # look for small images (width/height attributes sometimes present)\n",
    "    for im in card_tag.find_all(\"img\"):\n",
    "        w = im.get(\"width\")\n",
    "        h = im.get(\"height\")\n",
    "        src = im.get(\"src\") or im.get(\"data-src\")\n",
    "        if not src:\n",
    "            continue\n",
    "        # simple heuristic: logo images are usually small in pixel dims or have 'logo' in alt\n",
    "        if (w and int(w) < 100) or (h and int(h) < 100) or (im.get(\"alt\") and \"logo\" in im.get(\"alt\").lower()):\n",
    "            logo = src\n",
    "            break\n",
    "    if logo:\n",
    "        data[\"primary_article\"][\"source_logo\"] = logo\n",
    "\n",
    "    # 5) AUTHOR: google news often doesn't show author in the card; attempt find byline\n",
    "    byline = None\n",
    "    # Look for text that contains \"-\" separating source from title, or \"•\" bullets\n",
    "    # Or an element with 'aria-label' containing the source and author.\n",
    "    for el in card_tag.select(\"div, span\"):\n",
    "        txt = safe_text(el)\n",
    "        if not txt:\n",
    "            continue\n",
    "        if \"-\" in txt and len(txt.split(\"-\")) <= 3:\n",
    "            # assume part after dash may be author/source snippet - use first reasonable candidate\n",
    "            parts = [p.strip() for p in txt.split(\"-\")]\n",
    "            candidate = parts[-1]\n",
    "            if len(candidate.split()) <= 6 and not any(ch.isdigit() for ch in candidate[:3]):\n",
    "                byline = candidate\n",
    "                break\n",
    "    if byline:\n",
    "        data[\"primary_article\"][\"author\"] = byline\n",
    "\n",
    "    # 6) RELATED ARTICLES: Google topics often include a list of related <article> items inside card.\n",
    "    related = []\n",
    "    # Look for article tags inside the card (excluding the main one already used)\n",
    "    article_tags = card_tag.find_all(\"article\")\n",
    "    # if multiple article tags, assume first is primary, rest are related\n",
    "    if len(article_tags) > 1:\n",
    "        for art in article_tags[1:]:\n",
    "            ra = {}\n",
    "            # headline for related\n",
    "            h = art.find(\"h3\") or art.find(\"h4\")\n",
    "            ra['headline'] = safe_text(h) or \"N/A\"\n",
    "            a = art.find(\"a\", href=True)\n",
    "            ra['article_link'] = make_full_link(a['href']) if a else \"N/A\"\n",
    "            # source\n",
    "            s = art.find(\"img\")\n",
    "            ra['source_logo'] = s.get(\"src\") if s and s.get(\"src\") else \"N/A\"\n",
    "            # try a short text near article for source name\n",
    "            parent_divs = art.select(\"div, span\")\n",
    "            ra_source = None\n",
    "            for el in parent_divs[:10]:\n",
    "                txt = safe_text(el)\n",
    "                if txt and len(txt.split()) <= 5 and txt != ra['headline']:\n",
    "                    ra_source = txt\n",
    "                    break\n",
    "            ra['source_name'] = ra_source or \"N/A\"\n",
    "            ra['author'] = \"N/A\"\n",
    "            ra['publish_date'] = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "            related.append(ra)\n",
    "    else:\n",
    "        # sometimes related items are links or list items within the card\n",
    "        # try to find additional <a> elements which are not the primary link\n",
    "        anchors = card_tag.find_all(\"a\", href=True)\n",
    "        primary_href = data[\"primary_article\"].get(\"article_link\")\n",
    "        for a in anchors[1:6]:  # sample up to next 5 anchors\n",
    "            href = make_full_link(a[\"href\"])\n",
    "            text = safe_text(a)\n",
    "            if href == primary_href:\n",
    "                continue\n",
    "            related.append({\n",
    "                \"headline\": text or \"N/A\",\n",
    "                \"article_link\": href,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"N/A\",\n",
    "                \"author\": \"N/A\",\n",
    "                \"publish_date\": datetime.now().strftime(\"%d-%m-%Y\")\n",
    "            })\n",
    "\n",
    "    data[\"related_articles\"] = related\n",
    "    data[\"total_related_articles\"] = len(related)\n",
    "\n",
    "    # fill defaults if some keys are None\n",
    "    for k, v in data[\"primary_article\"].items():\n",
    "        if v is None:\n",
    "            data[\"primary_article\"][k] = \"N/A\"\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def scrape(url, file):\n",
    "    headers_list = []\n",
    "    fill_headers_list(headers_list)\n",
    "    soup = get_soup(url, headers_list)\n",
    "\n",
    "    # choose a flexible selector; google markup sometimes uses c-wiz or divs with certain classes\n",
    "    cards = soup.select('c-wiz[jsrenderer=\"ARwRbe\"], c-wiz[jsmodel=\"hc6Ubd\"], div[class*=\"WHE7ib\"]')\n",
    "\n",
    "    print(f\"Found {len(cards)} candidate cards for {url}\")\n",
    "\n",
    "    news_data = []\n",
    "    for idx, card in enumerate(cards):\n",
    "        try:\n",
    "            item = parse_card(card)\n",
    "            news_data.append(item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing card {idx}: {e}\")\n",
    "\n",
    "    # write json\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✅ File {file} saved successfully with {len(news_data)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b124067-cabc-4d04-be6d-7805aa364e34",
   "metadata": {},
   "source": [
    "## Get the entire data in a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aa539c10-9a39-4051-b23c-b08b9b8fa1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 candidate cards for https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File business_news.json saved successfully with 145 items.\n",
      "Found 121 candidate cards for https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRE55YXpBU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File india_news.json saved successfully with 121 items.\n",
      "Found 67 candidate cards for https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGRqTVhZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File technology_news.json saved successfully with 67 items.\n",
      "Found 73 candidate cards for https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File world_news.json saved successfully with 73 items.\n",
      "Found 207 candidate cards for https://news.google.com/topics/CAAqHAgKIhZDQklTQ2pvSWJHOWpZV3hmZGpJb0FBUAE/sections/CAQiTkNCSVNORG9JYkc5allXeGZkakpDRUd4dlkyRnNYM1l5WDNObFkzUnBiMjV5Q2hJSUwyMHZNR1JzZGpCNkNnb0lMMjB2TUdSc2RqQW9BQSowCAAqLAgKIiZDQklTRmpvSWJHOWpZV3hmZGpKNkNnb0lMMjB2TUdSc2RqQW9BQVABUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File local_news.json saved successfully with 207 items.\n",
      "Found 107 candidate cards for https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File entertainment_news.json saved successfully with 107 items.\n",
      "Found 103 candidate cards for https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File sports_news.json saved successfully with 103 items.\n",
      "Found 87 candidate cards for https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp0Y1RjU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File science_news.json saved successfully with 87 items.\n",
      "Found 143 candidate cards for https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "✅ File health_news.json saved successfully with 143 items.\n"
     ]
    }
   ],
   "source": [
    "business = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "india = \"https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRE55YXpBU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "technology = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGRqTVhZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "world = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "local = \"https://news.google.com/topics/CAAqHAgKIhZDQklTQ2pvSWJHOWpZV3hmZGpJb0FBUAE/sections/CAQiTkNCSVNORG9JYkc5allXeGZkakpDRUd4dlkyRnNYM1l5WDNObFkzUnBiMjV5Q2hJSUwyMHZNR1JzZGpCNkNnb0lMMjB2TUdSc2RqQW9BQSowCAAqLAgKIiZDQklTRmpvSWJHOWpZV3hmZGpKNkNnb0lMMjB2TUdSc2RqQW9BQVABUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "entertainment = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "sports = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "science = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp0Y1RjU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "health = \"https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "file = ['business_news.json','india_news.json', 'technology_news.json', 'world_news.json', 'local_news.json', 'entertainment_news.json', 'sports_news.json','science_news.json', 'health_news.json']\n",
    "url = [business, india, technology, world, local, entertainment, sports, science, health]\n",
    "n = len(file)\n",
    "for i in range(n):\n",
    "    u = url[i]\n",
    "    f = file[i]\n",
    "    scrape(u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f4b83-9e23-4bca-8ad0-c0ddd3e9e6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

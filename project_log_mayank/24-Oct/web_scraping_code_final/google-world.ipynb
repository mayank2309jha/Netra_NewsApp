{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3c8cd20-f3c7-4568-a6f9-ac303eb3b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdb0954b-b716-4612-9ec5-82e03ec1f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_headers_list(headers_list):\n",
    "    with open('headers.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            # Remove empty values to avoid sending None/empty headers\n",
    "            headers = {k: v for k, v in row.items() if v}\n",
    "            headers_list.append(headers)\n",
    "def get_soup(url, headers_list):\n",
    "    headers = random.choice(headers_list)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup\n",
    "def get_cards_by_headers(soup):\n",
    "    cards = soup.find_all(\"c-wiz\", {\n",
    "        \"jsrenderer\": \"ARwRbe\",\n",
    "        \"jsmodel\": \"hc6Ubd\",\n",
    "        \"class\": \"PO9Zff Ccj79 kUVvS\"\n",
    "    })\n",
    "    return cards\n",
    "def get_news_cards(cards):\n",
    "    return [str(card) for card in cards]\n",
    "def peel_outer_html(cards):\n",
    "    inner_divs = []\n",
    "    for card in cards:\n",
    "        inner_cwiz = card.find(\"c-wiz\", recursive=True)\n",
    "        if inner_cwiz:\n",
    "            first_div = inner_cwiz.find(\"div\", recursive=False)\n",
    "            if first_div:\n",
    "                second_div = first_div.find(\"div\", recursive=False)\n",
    "                if second_div:\n",
    "                    inner_divs.append(second_div)\n",
    "    return inner_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caa59b29-77b2-479d-b278-9fbf1048d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_to_dom(element):\n",
    "    \"\"\"Recursively convert a BeautifulSoup element to a nested dictionary structure.\"\"\"\n",
    "    dom = {}\n",
    "\n",
    "    for child in element.children:\n",
    "        # Skip text nodes that are just whitespace\n",
    "        if child.name is None:\n",
    "            text = child.strip()\n",
    "            if text:\n",
    "                dom[\"text\"] = text\n",
    "            continue\n",
    "\n",
    "        tag = child.name\n",
    "\n",
    "        # Determine value based on tag type\n",
    "        if tag == \"a\":\n",
    "            value = child.get(\"href\")\n",
    "        elif tag == \"img\":\n",
    "            value = child.get(\"src\")\n",
    "        elif tag == \"svg\":\n",
    "            value = str(child)\n",
    "        elif tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"span\", \"time\", \"button\"]:\n",
    "            value = child.decode_contents().strip()\n",
    "        else:\n",
    "            # If element has nested tags, recurse\n",
    "            nested = element_to_dom(child)\n",
    "            value = nested if nested else child.decode_contents().strip()\n",
    "\n",
    "        # Handle multiple same-tag children by converting value into a list\n",
    "        if tag in dom:\n",
    "            if isinstance(dom[tag], list):\n",
    "                dom[tag].append(value)\n",
    "            else:\n",
    "                dom[tag] = [dom[tag], value]\n",
    "        else:\n",
    "            dom[tag] = value\n",
    "\n",
    "    return dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41612e59-cb1a-4674-ab8c-1056800f3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(dom_structure, i, inner_divs_html):\n",
    "    \"\"\"\n",
    "    Extracts structured news data from a single dom_structure[i] element.\n",
    "    Returns a dictionary containing:\n",
    "      - primary_article (headline, link, author, etc.)\n",
    "      - related_articles (list of dictionaries)\n",
    "      - total_related_articles (count)\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://news.google.com\"\n",
    "    today_date = datetime.now().strftime(\"%d-%m-%Y\")  # current date in dd-mm-yyyy\n",
    "\n",
    "    def get_headline(i):\n",
    "        html = inner_divs_html[i]\n",
    "        temp_soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find the button\n",
    "        button = temp_soup.find('button')\n",
    "        \n",
    "        # Get the aria-label value\n",
    "        if button and button.has_attr('aria-label'):\n",
    "            return button['aria-label']\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    # --- PRIMARY ARTICLE ---\n",
    "    article = dom_structure.get(\"div\", {}).get(\"article\", {})\n",
    "    headline = get_headline(i)\n",
    "    news_data = {\n",
    "        'headline': headline if headline is not None else 'N/A',\n",
    "\n",
    "        'author': article.get('div', [])[2].get('div', {}).get('span', 'N/A')\n",
    "        if isinstance(article.get('div', []), list) and len(article.get('div', [])) > 2 else 'N/A',\n",
    "\n",
    "        'article_link': (\n",
    "            BASE_URL + article.get('a', '')\n",
    "            if article.get('a') not in (None, 'N/A') else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'featured_image': article.get('figure', {}).get('img', 'N/A'),\n",
    "\n",
    "        'source_logo': (\n",
    "            article.get('div', [])[1].get('div', [])[0].get('img', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'source_name': (\n",
    "            article.get('div', [])[1]\n",
    "            .get('div', [])[0]\n",
    "            .get('div', {})\n",
    "            .get('div', {})\n",
    "            .get('text', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        # --- NEW FIELD ---\n",
    "        'publish_date': today_date\n",
    "    }\n",
    "\n",
    "    # --- RELATED ARTICLES ---\n",
    "    related_articles = []\n",
    "    possible_related = dom_structure.get(\"div\", {}).get(\"div\", {}).get(\"article\", [])\n",
    "\n",
    "    if isinstance(possible_related, list):\n",
    "        for article in possible_related:\n",
    "            if not isinstance(article, dict):\n",
    "                continue\n",
    "\n",
    "            divs = article.get('div', [])\n",
    "            if not isinstance(divs, list) or len(divs) < 3:\n",
    "                continue\n",
    "\n",
    "            raw_link = article.get('a', '')\n",
    "            full_link = BASE_URL + raw_link if raw_link not in (None, 'N/A') else 'N/A'\n",
    "\n",
    "            article_data = {\n",
    "                'headline': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'author': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'article_link': full_link,\n",
    "                'source_logo': (\n",
    "                    divs[1].get('div', [])[0].get('img', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "                'source_name': (\n",
    "                    divs[1].get('div', [])[0]\n",
    "                    .get('div', {})\n",
    "                    .get('div', {})\n",
    "                    .get('text', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "\n",
    "                # --- NEW FIELD ---\n",
    "                'publish_date': today_date\n",
    "            }\n",
    "            related_articles.append(article_data)\n",
    "\n",
    "    # --- FINAL OUTPUT ---\n",
    "    complete_news_data = {\n",
    "        'primary_article': news_data,\n",
    "        'related_articles': related_articles,\n",
    "        'total_related_articles': len(related_articles)\n",
    "    }\n",
    "\n",
    "    return complete_news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa1c6e1a-c16a-48d9-affd-b0f11150988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url, file):\n",
    "    headers_list = []\n",
    "    fill_headers_list(headers_list)\n",
    "    soup = get_soup(url, headers_list)\n",
    "    cards = get_cards_by_headers(soup)\n",
    "    news_cards = get_news_cards(cards)\n",
    "    inner_divs = peel_outer_html(cards)\n",
    "    inner_divs_html = [str(div) for div in inner_divs]\n",
    "    dom_structures = []\n",
    "    for html in inner_divs_html:\n",
    "        temp_soup = BeautifulSoup(html, \"html.parser\")\n",
    "        dom_dict = element_to_dom(temp_soup)\n",
    "        dom_structures.append(dom_dict)\n",
    "    news_data = []\n",
    "\n",
    "    for i, dom in enumerate(dom_structures):\n",
    "        try:\n",
    "            news_item = get_news_data(dom, i, inner_divs_html)\n",
    "            news_data.append(news_item)\n",
    "            # print(f\"Processed item {i + 1}/{len(dom_structures)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {i + 1}: {e}\")\n",
    "    \n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_data, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\n✅ File \"+file+\" saved successfully in the working directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3d51ee1-ebe3-4245-82d0-941aec1b8450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ File world_news.json saved successfully in the working directory!\n"
     ]
    }
   ],
   "source": [
    "world = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(world, 'world_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d6aba-058f-4f89-86f1-17c8a95078f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
